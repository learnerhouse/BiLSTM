{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bilstm + multi classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/learnerhouse/BiLSTM/blob/master/Bilstm_%2B_multi_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO8bmcqoZG8i",
        "colab_type": "text"
      },
      "source": [
        "# Bilstm + multi classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ7c2-KiZRDU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42cb807b-52c1-41e9-aaa2-69c60156e492"
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1072607b10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ppODGhVZVo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec,1)\n",
        "    return idx.item()\n",
        "\n",
        "def prepare_sequence(seq, to_ix,max_len=-1):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    if max_len != -1:\n",
        "        idxs.extend([0]*(max_len-len(seq)))\n",
        "    return torch.tensor(idxs, dtype=torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I38CndgZhQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiLSTM_MCLS(nn.Module):\n",
        "\n",
        "    # 初始化变量以及定义几个隐层\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim,label_tree,seq_max_len=10):\n",
        "        super(BiLSTM_MCLS, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.label_tree = label_tree\n",
        "        self.seq_max_len = seq_max_len\n",
        "        self.height = len(self.label_tree)\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "        self.word_embeds = nn.Embedding(vocab_size,embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True,batch_first=True)\n",
        "        \n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden_cls = []\n",
        "        self.label_cls  = []\n",
        "        # for index,c_v in enumerate(label_tree):\n",
        "        #     self.hidden_cls.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "        #     self.label_cls.append(nn.Linear(hidden_dim, len(c_v)))\n",
        "\n",
        "        self.hidden_cls1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.hidden_cls2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.label_cls1  = nn.Linear(hidden_dim, len(label_tree[0]))\n",
        "        self.label_cls2  = nn.Linear(hidden_dim, len(label_tree[1]))\n",
        "        \n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dim//2, self.hidden_dim//2),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.hidden = self.init_hidden()\n",
        "    \n",
        "    # 初始化隐层\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2))\n",
        "\n",
        "    # 获取特征向量\n",
        "    def _get_lstm_feature(self,sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(1,len(sentence), -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        return lstm_out\n",
        "    \n",
        "    def attention_net_with_w(self, lstm_out):\n",
        "        '''\n",
        "        :param lstm_out: [batch_size, time_step, hidden_dims * num_directions(=2)]\n",
        "        :return:\n",
        "        '''\n",
        "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
        "        # h [batch_size, time_step, hidden_dims]\n",
        "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
        "        # atten_w [batch_size, time_step, hidden_dims]\n",
        "        atten_w = self.attention_layer(h)\n",
        "        # m [batch_size, time_step, hidden_dims]\n",
        "        m = nn.Tanh()(h)\n",
        "        # atten_context [batch_size, time_step, time_step]\n",
        "        atten_context = torch.bmm(m, atten_w.transpose(1, 2))\n",
        "        # softmax_w [batch_size, time_step, time_step]\n",
        "        softmax_w = F.softmax(atten_context, dim=-1)\n",
        "        # context [batch_size, hidden_dims, time_step]\n",
        "        context = torch.bmm(h.transpose(1,2), softmax_w)\n",
        "        context_with_attn = h.transpose(1, 2) + context\n",
        "        # result [batch_size, hidden_dims]\n",
        "        # result = torch.sum(context, dim=-1)\n",
        "        result = torch.sum(context_with_attn, dim=-1)\n",
        "        return result\n",
        "\n",
        "    # 获取多级分类器的标签向量\n",
        "    def _get_multi_label(self,feature):\n",
        "        # for index,layer in enumerate(self.hidden_cls):\n",
        "        #     if index >= 1:\n",
        "        #         tmp_hidden = torch.cat((tmp_hidden,torch.sigmoid(layer(feature))))\n",
        "        #         fixed_hidden = torch.sigmoid(self.label_cls[index](tmp_hidden)) \n",
        "        #         multi_label[index] = fixed_hidden[-1]\n",
        "        #     else:\n",
        "        #         tmp_hidden = torch.sigmoid(layer(feature)) \n",
        "        #         fixed_hidden = torch.sigmoid(self.label_cls[index](tmp_hidden))\n",
        "        #         multi_label[index] = fixed_hidden[-1]\n",
        "        \n",
        "        # hidden_att = self.attention_net_with_w(feature.view(1,10,20).permute(1, 0, 2))\n",
        "        # print (hidden_att.shape)  # 10*10\n",
        "        \n",
        "        tmp_hidden1 = torch.relu(self.hidden_cls1(feature))\n",
        "        fixed_hidden1 = torch.sigmoid(self.label_cls1(tmp_hidden1))\n",
        "\n",
        "        tmp_hidden2 = torch.cat((tmp_hidden1,torch.sigmoid(self.hidden_cls2(feature))))\n",
        "        fixed_hidden2 = torch.sigmoid(self.label_cls2(tmp_hidden2))\n",
        "\n",
        "        return fixed_hidden1,fixed_hidden2,[argmax(fixed_hidden1.view(1,-1)),argmax(fixed_hidden2.view(1,-1))]\n",
        "    \n",
        "    # 计算交叉熵\n",
        "    def my_coss_entropy(self,sentence,labels):\n",
        "        lstm_feature = self._get_lstm_feature(sentence)\n",
        "        pre_label1,pre_label2,label_ids = self._get_multi_label(lstm_feature)\n",
        "        score = self.loss(pre_label1.view(1,-1),labels[0].view(1)) + self.loss(pre_label2.view(1,-1),labels[1].view(1))\n",
        "        return score\n",
        "    \n",
        "    # 计算前向网络，给出预测标签的结果\n",
        "    def forward (self,sentence):\n",
        "        # print (\"句子编码:\",sentence)\n",
        "        lstm_feature = self._get_lstm_feature(sentence)\n",
        "        # print (\"句子嵌入:\",lstm_feature)\n",
        "        label1,label2,label_ids = self._get_multi_label(lstm_feature)\n",
        "        return label_ids\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcbHCG3IC60L",
        "colab_type": "text"
      },
      "source": [
        "测试代码"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhsJNNOXC5iS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ac82cef6-af69-4d6d-f737-a90ceb4039b6"
      },
      "source": [
        "EMBEDDING_DIM = 10\n",
        "HIDDEN_DIM = 10\n",
        "max_len = -1\n",
        "\n",
        "# 标签树\n",
        "label_tree = [[\"nature\",\"science\"],[\"fruit\",\"company\"]]\n",
        "\n",
        "# Make up some training data\n",
        "training_data = [(\n",
        "    \"How much is the apple\".split(),\n",
        "    \"nature_fruit\".split(\"_\")\n",
        "), (\n",
        "    \"Apple is a great company in the world\".split(),\n",
        "    \"science_company\".split(\"_\")\n",
        ")]\n",
        "\n",
        "# 词嵌入\n",
        "word_to_ix = {}\n",
        "for sentence, tags in training_data:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "# 标签编码\n",
        "def label_encode(label_names,label_tree):\n",
        "    label_vectors = []\n",
        "    label_ids = []\n",
        "    for index,label in enumerate (label_names):\n",
        "        vector = [0] * len(label_tree[index])\n",
        "        vector[label_tree[index].index(label)] = 1\n",
        "        label_vectors.append(vector)\n",
        "        label_ids.append(label_tree[index].index(label))\n",
        "    return label_vectors,label_ids\n",
        "\n",
        "test_tag,_ = label_encode(\"science_company\".split(\"_\"),label_tree)\n",
        "\n",
        "model = BiLSTM_MCLS(len(word_to_ix), EMBEDDING_DIM, HIDDEN_DIM,label_tree)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "# Check predictions before training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[1][0], word_to_ix,max_len=max_len)\n",
        "    print (precheck_sent)\n",
        "    precheck_tags = torch.tensor(test_tag, dtype=torch.long)\n",
        "    print(model(precheck_sent))\n",
        "\n",
        "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
        "for epoch in range(\n",
        "        800):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    for sentence, tags in training_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix,max_len=max_len)\n",
        "        _,tags_id = label_encode(tags,label_tree)\n",
        "        targets = torch.tensor(tags_id, dtype=torch.long)\n",
        "        # print (sentence, targets)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.my_coss_entropy(sentence_in, targets)\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        if epoch % 10 == 0: print (loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Check predictions after training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix,max_len=max_len)\n",
        "    print(model(precheck_sent))\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 5,  2,  6,  7,  8,  9,  3, 10])\n",
            "[9, 2]\n",
            "tensor(5.2797, grad_fn=<AddBackward0>)\n",
            "tensor(6.2059, grad_fn=<AddBackward0>)\n",
            "tensor(5.2502, grad_fn=<AddBackward0>)\n",
            "tensor(6.1544, grad_fn=<AddBackward0>)\n",
            "tensor(5.2105, grad_fn=<AddBackward0>)\n",
            "tensor(6.1239, grad_fn=<AddBackward0>)\n",
            "tensor(5.1951, grad_fn=<AddBackward0>)\n",
            "tensor(6.0954, grad_fn=<AddBackward0>)\n",
            "tensor(5.1316, grad_fn=<AddBackward0>)\n",
            "tensor(6.0417, grad_fn=<AddBackward0>)\n",
            "tensor(5.0851, grad_fn=<AddBackward0>)\n",
            "tensor(5.9818, grad_fn=<AddBackward0>)\n",
            "tensor(5.0695, grad_fn=<AddBackward0>)\n",
            "tensor(5.8686, grad_fn=<AddBackward0>)\n",
            "tensor(5.0346, grad_fn=<AddBackward0>)\n",
            "tensor(5.7419, grad_fn=<AddBackward0>)\n",
            "tensor(4.8188, grad_fn=<AddBackward0>)\n",
            "tensor(5.5339, grad_fn=<AddBackward0>)\n",
            "tensor(4.9406, grad_fn=<AddBackward0>)\n",
            "tensor(5.2692, grad_fn=<AddBackward0>)\n",
            "tensor(4.5735, grad_fn=<AddBackward0>)\n",
            "tensor(4.9892, grad_fn=<AddBackward0>)\n",
            "tensor(4.4322, grad_fn=<AddBackward0>)\n",
            "tensor(4.7848, grad_fn=<AddBackward0>)\n",
            "tensor(4.3474, grad_fn=<AddBackward0>)\n",
            "tensor(4.6698, grad_fn=<AddBackward0>)\n",
            "tensor(4.2560, grad_fn=<AddBackward0>)\n",
            "tensor(4.5938, grad_fn=<AddBackward0>)\n",
            "tensor(4.1854, grad_fn=<AddBackward0>)\n",
            "tensor(4.5658, grad_fn=<AddBackward0>)\n",
            "tensor(4.0911, grad_fn=<AddBackward0>)\n",
            "tensor(4.5603, grad_fn=<AddBackward0>)\n",
            "tensor(4.0034, grad_fn=<AddBackward0>)\n",
            "tensor(4.5164, grad_fn=<AddBackward0>)\n",
            "tensor(3.9567, grad_fn=<AddBackward0>)\n",
            "tensor(4.5133, grad_fn=<AddBackward0>)\n",
            "tensor(3.8600, grad_fn=<AddBackward0>)\n",
            "tensor(4.4821, grad_fn=<AddBackward0>)\n",
            "tensor(3.8056, grad_fn=<AddBackward0>)\n",
            "tensor(4.4805, grad_fn=<AddBackward0>)\n",
            "tensor(3.7779, grad_fn=<AddBackward0>)\n",
            "tensor(4.4674, grad_fn=<AddBackward0>)\n",
            "tensor(3.7641, grad_fn=<AddBackward0>)\n",
            "tensor(4.4488, grad_fn=<AddBackward0>)\n",
            "tensor(3.7394, grad_fn=<AddBackward0>)\n",
            "tensor(4.4510, grad_fn=<AddBackward0>)\n",
            "tensor(3.6982, grad_fn=<AddBackward0>)\n",
            "tensor(4.4410, grad_fn=<AddBackward0>)\n",
            "tensor(3.6984, grad_fn=<AddBackward0>)\n",
            "tensor(4.4380, grad_fn=<AddBackward0>)\n",
            "tensor(3.6656, grad_fn=<AddBackward0>)\n",
            "tensor(4.4327, grad_fn=<AddBackward0>)\n",
            "tensor(3.6437, grad_fn=<AddBackward0>)\n",
            "tensor(4.4246, grad_fn=<AddBackward0>)\n",
            "tensor(3.6435, grad_fn=<AddBackward0>)\n",
            "tensor(4.4242, grad_fn=<AddBackward0>)\n",
            "tensor(3.6217, grad_fn=<AddBackward0>)\n",
            "tensor(4.4185, grad_fn=<AddBackward0>)\n",
            "tensor(3.6188, grad_fn=<AddBackward0>)\n",
            "tensor(4.4175, grad_fn=<AddBackward0>)\n",
            "tensor(3.6086, grad_fn=<AddBackward0>)\n",
            "tensor(4.4145, grad_fn=<AddBackward0>)\n",
            "tensor(3.6040, grad_fn=<AddBackward0>)\n",
            "tensor(4.4152, grad_fn=<AddBackward0>)\n",
            "tensor(3.5947, grad_fn=<AddBackward0>)\n",
            "tensor(4.4105, grad_fn=<AddBackward0>)\n",
            "tensor(3.5848, grad_fn=<AddBackward0>)\n",
            "tensor(4.4116, grad_fn=<AddBackward0>)\n",
            "tensor(3.5858, grad_fn=<AddBackward0>)\n",
            "tensor(4.4107, grad_fn=<AddBackward0>)\n",
            "tensor(3.5879, grad_fn=<AddBackward0>)\n",
            "tensor(4.4090, grad_fn=<AddBackward0>)\n",
            "tensor(3.5781, grad_fn=<AddBackward0>)\n",
            "tensor(4.4129, grad_fn=<AddBackward0>)\n",
            "tensor(3.5767, grad_fn=<AddBackward0>)\n",
            "tensor(4.4056, grad_fn=<AddBackward0>)\n",
            "tensor(3.5662, grad_fn=<AddBackward0>)\n",
            "tensor(4.4062, grad_fn=<AddBackward0>)\n",
            "tensor(3.5684, grad_fn=<AddBackward0>)\n",
            "tensor(4.4056, grad_fn=<AddBackward0>)\n",
            "tensor(3.5707, grad_fn=<AddBackward0>)\n",
            "tensor(4.4046, grad_fn=<AddBackward0>)\n",
            "tensor(3.5635, grad_fn=<AddBackward0>)\n",
            "tensor(4.4069, grad_fn=<AddBackward0>)\n",
            "tensor(3.5650, grad_fn=<AddBackward0>)\n",
            "tensor(4.4043, grad_fn=<AddBackward0>)\n",
            "tensor(3.5626, grad_fn=<AddBackward0>)\n",
            "tensor(4.4042, grad_fn=<AddBackward0>)\n",
            "tensor(3.5592, grad_fn=<AddBackward0>)\n",
            "tensor(4.4037, grad_fn=<AddBackward0>)\n",
            "tensor(3.5581, grad_fn=<AddBackward0>)\n",
            "tensor(4.4016, grad_fn=<AddBackward0>)\n",
            "tensor(3.5564, grad_fn=<AddBackward0>)\n",
            "tensor(4.4024, grad_fn=<AddBackward0>)\n",
            "tensor(3.5575, grad_fn=<AddBackward0>)\n",
            "tensor(4.4038, grad_fn=<AddBackward0>)\n",
            "tensor(3.5586, grad_fn=<AddBackward0>)\n",
            "tensor(4.4035, grad_fn=<AddBackward0>)\n",
            "tensor(3.5526, grad_fn=<AddBackward0>)\n",
            "tensor(4.4044, grad_fn=<AddBackward0>)\n",
            "tensor(3.5563, grad_fn=<AddBackward0>)\n",
            "tensor(4.4008, grad_fn=<AddBackward0>)\n",
            "tensor(3.5531, grad_fn=<AddBackward0>)\n",
            "tensor(4.4000, grad_fn=<AddBackward0>)\n",
            "tensor(3.5529, grad_fn=<AddBackward0>)\n",
            "tensor(4.4000, grad_fn=<AddBackward0>)\n",
            "tensor(3.5572, grad_fn=<AddBackward0>)\n",
            "tensor(4.3999, grad_fn=<AddBackward0>)\n",
            "tensor(3.5517, grad_fn=<AddBackward0>)\n",
            "tensor(4.3996, grad_fn=<AddBackward0>)\n",
            "tensor(3.5517, grad_fn=<AddBackward0>)\n",
            "tensor(4.4008, grad_fn=<AddBackward0>)\n",
            "tensor(3.5510, grad_fn=<AddBackward0>)\n",
            "tensor(4.4009, grad_fn=<AddBackward0>)\n",
            "tensor(3.5515, grad_fn=<AddBackward0>)\n",
            "tensor(4.3995, grad_fn=<AddBackward0>)\n",
            "tensor(3.5539, grad_fn=<AddBackward0>)\n",
            "tensor(4.3988, grad_fn=<AddBackward0>)\n",
            "tensor(3.5541, grad_fn=<AddBackward0>)\n",
            "tensor(4.3989, grad_fn=<AddBackward0>)\n",
            "tensor(3.5522, grad_fn=<AddBackward0>)\n",
            "tensor(4.3986, grad_fn=<AddBackward0>)\n",
            "tensor(3.5510, grad_fn=<AddBackward0>)\n",
            "tensor(4.3987, grad_fn=<AddBackward0>)\n",
            "tensor(3.5484, grad_fn=<AddBackward0>)\n",
            "tensor(4.3980, grad_fn=<AddBackward0>)\n",
            "tensor(3.5510, grad_fn=<AddBackward0>)\n",
            "tensor(4.3989, grad_fn=<AddBackward0>)\n",
            "tensor(3.5492, grad_fn=<AddBackward0>)\n",
            "tensor(4.3993, grad_fn=<AddBackward0>)\n",
            "tensor(3.5478, grad_fn=<AddBackward0>)\n",
            "tensor(4.3978, grad_fn=<AddBackward0>)\n",
            "tensor(3.5476, grad_fn=<AddBackward0>)\n",
            "tensor(4.3985, grad_fn=<AddBackward0>)\n",
            "tensor(3.5470, grad_fn=<AddBackward0>)\n",
            "tensor(4.3977, grad_fn=<AddBackward0>)\n",
            "tensor(3.5481, grad_fn=<AddBackward0>)\n",
            "tensor(4.3984, grad_fn=<AddBackward0>)\n",
            "tensor(3.5470, grad_fn=<AddBackward0>)\n",
            "tensor(4.3978, grad_fn=<AddBackward0>)\n",
            "tensor(3.5475, grad_fn=<AddBackward0>)\n",
            "tensor(4.3977, grad_fn=<AddBackward0>)\n",
            "tensor(3.5476, grad_fn=<AddBackward0>)\n",
            "tensor(4.3971, grad_fn=<AddBackward0>)\n",
            "tensor(3.5468, grad_fn=<AddBackward0>)\n",
            "tensor(4.3978, grad_fn=<AddBackward0>)\n",
            "tensor(3.5477, grad_fn=<AddBackward0>)\n",
            "tensor(4.3972, grad_fn=<AddBackward0>)\n",
            "tensor(3.5471, grad_fn=<AddBackward0>)\n",
            "tensor(4.3971, grad_fn=<AddBackward0>)\n",
            "tensor(3.5466, grad_fn=<AddBackward0>)\n",
            "tensor(4.3971, grad_fn=<AddBackward0>)\n",
            "tensor(3.5457, grad_fn=<AddBackward0>)\n",
            "tensor(4.3986, grad_fn=<AddBackward0>)\n",
            "tensor(3.5458, grad_fn=<AddBackward0>)\n",
            "tensor(4.3970, grad_fn=<AddBackward0>)\n",
            "tensor(3.5453, grad_fn=<AddBackward0>)\n",
            "tensor(4.3988, grad_fn=<AddBackward0>)\n",
            "tensor(3.5457, grad_fn=<AddBackward0>)\n",
            "tensor(4.3964, grad_fn=<AddBackward0>)\n",
            "[0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af0myeIj18s6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4274fc45-dc67-41df-c995-abbb3786bcc2"
      },
      "source": [
        "m = nn.MaxPool2d(3, stride=2)\n",
        "input = torch.randn(20, 16, 50)\n",
        "print (input.shape)\n",
        "output = m(input)\n",
        "print (output.shape)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([20, 16, 50])\n",
            "torch.Size([20, 7, 24])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VicZ5u1a2GZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}