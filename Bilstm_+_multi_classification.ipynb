{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bilstm + multi classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/learnerhouse/BiLSTM/blob/master/Bilstm_%2B_multi_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO8bmcqoZG8i",
        "colab_type": "text"
      },
      "source": [
        "# Bilstm + multi classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ7c2-KiZRDU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42cb807b-52c1-41e9-aaa2-69c60156e492"
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1072607b10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ppODGhVZVo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec,1)\n",
        "    return idx.item()\n",
        "\n",
        "def prepare_sequence(seq, to_ix,max_len=-1):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    if max_len != -1:\n",
        "        idxs.extend([0]*(max_len-len(seq)))\n",
        "    return torch.tensor(idxs, dtype=torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I38CndgZhQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiLSTM_MCLS(nn.Module):\n",
        "\n",
        "    # 初始化变量以及定义几个隐层\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim,label_tree,seq_max_len=10):\n",
        "        super(BiLSTM_MCLS, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.label_tree = label_tree\n",
        "        self.seq_max_len = seq_max_len\n",
        "        self.height = len(self.label_tree)\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "        self.word_embeds = nn.Embedding(vocab_size,embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True,batch_first=True)\n",
        "        \n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden_cls = []\n",
        "        self.label_cls  = []\n",
        "        # for index,c_v in enumerate(label_tree):\n",
        "        #     self.hidden_cls.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "        #     self.label_cls.append(nn.Linear(hidden_dim, len(c_v)))\n",
        "\n",
        "        self.hidden_cls1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.hidden_cls2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.label_cls1  = nn.Linear(seq_max_len*hidden_dim, len(label_tree[0]))\n",
        "        self.label_cls2  = nn.Linear(seq_max_len*hidden_dim*2, len(label_tree[1]))\n",
        "        \n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dim//2, self.hidden_dim//2),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.hidden = self.init_hidden()\n",
        "    \n",
        "    # 初始化隐层\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2))\n",
        "\n",
        "    # 获取特征向量\n",
        "    def _get_lstm_feature(self,sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(1,len(sentence), -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        return lstm_out\n",
        "    \n",
        "    def attention_net_with_w(self, lstm_out):\n",
        "        '''\n",
        "        :param lstm_out: [batch_size, time_step, hidden_dims * num_directions(=2)]\n",
        "        :return:\n",
        "        '''\n",
        "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
        "        # h [batch_size, time_step, hidden_dims]\n",
        "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
        "        # atten_w [batch_size, time_step, hidden_dims]\n",
        "        atten_w = self.attention_layer(h)\n",
        "        # m [batch_size, time_step, hidden_dims]\n",
        "        m = nn.Tanh()(h)\n",
        "        # atten_context [batch_size, time_step, time_step]\n",
        "        atten_context = torch.bmm(m, atten_w.transpose(1, 2))\n",
        "        # softmax_w [batch_size, time_step, time_step]\n",
        "        softmax_w = F.softmax(atten_context, dim=-1)\n",
        "        # context [batch_size, hidden_dims, time_step]\n",
        "        context = torch.bmm(h.transpose(1,2), softmax_w)\n",
        "        context_with_attn = h.transpose(1, 2) + context\n",
        "        # result [batch_size, hidden_dims]\n",
        "        # result = torch.sum(context, dim=-1)\n",
        "        result = torch.sum(context_with_attn, dim=-1)\n",
        "        return result\n",
        "\n",
        "    # 获取多级分类器的标签向量\n",
        "    def _get_multi_label(self,feature):\n",
        "        # for index,layer in enumerate(self.hidden_cls):\n",
        "        #     if index >= 1:\n",
        "        #         tmp_hidden = torch.cat((tmp_hidden,torch.sigmoid(layer(feature))))\n",
        "        #         fixed_hidden = torch.sigmoid(self.label_cls[index](tmp_hidden)) \n",
        "        #         multi_label[index] = fixed_hidden[-1]\n",
        "        #     else:\n",
        "        #         tmp_hidden = torch.sigmoid(layer(feature)) \n",
        "        #         fixed_hidden = torch.sigmoid(self.label_cls[index](tmp_hidden))\n",
        "        #         multi_label[index] = fixed_hidden[-1]\n",
        "        \n",
        "        # hidden_att = self.attention_net_with_w(feature.view(1,10,20).permute(1, 0, 2))\n",
        "        # print (hidden_att.shape)  # 10*10\n",
        "        \n",
        "        hidden_att = feature\n",
        "        tmp_hidden1 = torch.relu(self.hidden_cls1(hidden_att)).view(1,-1)\n",
        "        fixed_hidden1 = torch.relu(self.label_cls1(tmp_hidden1))\n",
        "\n",
        "        tmp_hidden2 = torch.cat((tmp_hidden1,torch.sigmoid(self.hidden_cls2(hidden_att)).view(1,-1))).view(1,-1)\n",
        "        fixed_hidden2 = torch.relu(self.label_cls2(tmp_hidden2))\n",
        "\n",
        "        return fixed_hidden1,fixed_hidden2,[argmax(fixed_hidden1.view(1,-1)),argmax(fixed_hidden2.view(1,-1))]\n",
        "    \n",
        "    # 计算交叉熵\n",
        "    def my_coss_entropy(self,sentence,labels):\n",
        "        lstm_feature = self._get_lstm_feature(sentence)\n",
        "        pre_label1,pre_label2,label_ids = self._get_multi_label(lstm_feature)\n",
        "        score = self.loss(pre_label1.view(1,-1),labels[0].view(1)) + self.loss(pre_label2.view(1,-1),labels[1].view(1))\n",
        "        return score\n",
        "    \n",
        "    # 计算前向网络，给出预测标签的结果\n",
        "    def forward (self,sentence):\n",
        "        # print (\"句子编码:\",sentence)\n",
        "        lstm_feature = self._get_lstm_feature(sentence)\n",
        "        # print (\"句子嵌入:\",lstm_feature)\n",
        "        label1,label2,label_ids = self._get_multi_label(lstm_feature)\n",
        "        return label_ids\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcbHCG3IC60L",
        "colab_type": "text"
      },
      "source": [
        "测试代码"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhsJNNOXC5iS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "0f010805-707e-4a98-d2a4-98c589e075f5"
      },
      "source": [
        "EMBEDDING_DIM = 20\n",
        "HIDDEN_DIM = 20\n",
        "max_len = 10\n",
        "\n",
        "# 标签树\n",
        "label_tree = [[\"nature\",\"science\"],[\"fruit\",\"company\"]]\n",
        "\n",
        "# Make up some training data\n",
        "training_data = [(\n",
        "    \"How much is the apple\".split(),\n",
        "    \"nature_fruit\".split(\"_\")\n",
        "), (\n",
        "    \"Apple is a great company in the world\".split(),\n",
        "    \"science_company\".split(\"_\")\n",
        ")]\n",
        "\n",
        "# 词嵌入\n",
        "word_to_ix = {}\n",
        "for sentence, tags in training_data:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "# 标签编码\n",
        "def label_encode(label_names,label_tree):\n",
        "    label_vectors = []\n",
        "    label_ids = []\n",
        "    for index,label in enumerate (label_names):\n",
        "        vector = [0] * len(label_tree[index])\n",
        "        vector[label_tree[index].index(label)] = 1\n",
        "        label_vectors.append(vector)\n",
        "        label_ids.append(label_tree[index].index(label))\n",
        "    return label_vectors,label_ids\n",
        "\n",
        "test_tag,_ = label_encode(\"science_company\".split(\"_\"),label_tree)\n",
        "\n",
        "model = BiLSTM_MCLS(len(word_to_ix), EMBEDDING_DIM, HIDDEN_DIM,label_tree)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "# Check predictions before training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[1][0], word_to_ix,max_len=max_len)\n",
        "    print (precheck_sent)\n",
        "    precheck_tags = torch.tensor(test_tag, dtype=torch.long)\n",
        "    print(model(precheck_sent))\n",
        "\n",
        "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
        "for epoch in range(\n",
        "        200):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    for sentence, tags in training_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix,max_len=max_len)\n",
        "        _,tags_id = label_encode(tags,label_tree)\n",
        "        targets = torch.tensor(tags_id, dtype=torch.long)\n",
        "        # print (sentence, targets)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.my_coss_entropy(sentence_in, targets)\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        if epoch % 10 == 0: print (loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Check predictions after training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix,max_len=max_len)\n",
        "    print(model(precheck_sent))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 5,  2,  6,  7,  8,  9,  3, 10,  0,  0])\n",
            "[1, 0]\n",
            "tensor(1.2632, grad_fn=<AddBackward0>)\n",
            "tensor(1.6147, grad_fn=<AddBackward0>)\n",
            "tensor(1.3199, grad_fn=<AddBackward0>)\n",
            "tensor(1.5310, grad_fn=<AddBackward0>)\n",
            "tensor(1.3048, grad_fn=<AddBackward0>)\n",
            "tensor(1.5023, grad_fn=<AddBackward0>)\n",
            "tensor(1.3101, grad_fn=<AddBackward0>)\n",
            "tensor(1.4658, grad_fn=<AddBackward0>)\n",
            "tensor(1.3348, grad_fn=<AddBackward0>)\n",
            "tensor(1.4459, grad_fn=<AddBackward0>)\n",
            "tensor(1.3250, grad_fn=<AddBackward0>)\n",
            "tensor(1.4256, grad_fn=<AddBackward0>)\n",
            "tensor(1.3619, grad_fn=<AddBackward0>)\n",
            "tensor(1.4274, grad_fn=<AddBackward0>)\n",
            "tensor(1.3333, grad_fn=<AddBackward0>)\n",
            "tensor(1.3952, grad_fn=<AddBackward0>)\n",
            "tensor(1.3404, grad_fn=<AddBackward0>)\n",
            "tensor(1.3941, grad_fn=<AddBackward0>)\n",
            "tensor(1.3357, grad_fn=<AddBackward0>)\n",
            "tensor(1.3685, grad_fn=<AddBackward0>)\n",
            "tensor(1.3144, grad_fn=<AddBackward0>)\n",
            "tensor(1.3838, grad_fn=<AddBackward0>)\n",
            "tensor(1.3081, grad_fn=<AddBackward0>)\n",
            "tensor(1.3932, grad_fn=<AddBackward0>)\n",
            "tensor(1.2804, grad_fn=<AddBackward0>)\n",
            "tensor(1.3541, grad_fn=<AddBackward0>)\n",
            "tensor(1.3341, grad_fn=<AddBackward0>)\n",
            "tensor(1.3768, grad_fn=<AddBackward0>)\n",
            "tensor(1.3035, grad_fn=<AddBackward0>)\n",
            "tensor(1.3434, grad_fn=<AddBackward0>)\n",
            "tensor(1.2870, grad_fn=<AddBackward0>)\n",
            "tensor(1.3894, grad_fn=<AddBackward0>)\n",
            "tensor(1.2891, grad_fn=<AddBackward0>)\n",
            "tensor(1.3601, grad_fn=<AddBackward0>)\n",
            "tensor(1.2839, grad_fn=<AddBackward0>)\n",
            "tensor(1.3489, grad_fn=<AddBackward0>)\n",
            "tensor(1.2594, grad_fn=<AddBackward0>)\n",
            "tensor(1.3502, grad_fn=<AddBackward0>)\n",
            "tensor(1.2858, grad_fn=<AddBackward0>)\n",
            "tensor(1.3402, grad_fn=<AddBackward0>)\n",
            "[0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}